{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "import re\n",
    "import itertools\n",
    "from itertools import compress\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pdfplumber\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from gensim.corpora import MalletCorpus\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(20,12)})\n",
    "\n",
    "data_path = os.path.join('..', 'data')\n",
    "pdf_path = os.path.join(data_path, 'pdf')\n",
    "out_path = os.path.join('..', 'output')\n",
    "datafile = 'drr_scrape2021-07-08.json'\n",
    "datafile_tokenized = 'drr_scrape2021-07-08_tokenized.json'\n",
    "datafile_es = 'drr_scrape2021-07-08_es.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    \n",
    "if not os.path.isdir(pdf_path):\n",
    "    os.mkdir(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data consists of 284 texts in total. \n",
      "133 texts are from UN DDR (https://www.unddr.org). \n",
      "151 texts are from DRMKC EU (https://drmkc.jrc.ec.europa.eu)\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "path = os.path.join(data_path, datafile_tokenized)\n",
    "is_tokenized = True\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    path = os.path.join(data_path, datafile)\n",
    "    is_tokenized = False\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter data\n",
    "\n",
    "data_unique = []\n",
    "urls_added = []\n",
    "\n",
    "for entry in data:\n",
    "    if entry['url'] in urls_added:\n",
    "        continue\n",
    "    else:\n",
    "        data_unique.append(entry)\n",
    "        urls_added.append(entry['url'])\n",
    "    \n",
    "data = data_unique\n",
    "\n",
    "for entry in data:\n",
    "    if entry['domain_url'] == 'https://www.unddr.org':\n",
    "        entry['org'] = 'unddr'\n",
    "    elif entry['domain_url'] == 'https://drmkc.jrc.ec.europa.eu':\n",
    "        entry['org'] = 'drmkc'\n",
    "    else:\n",
    "        entry['org'] = ''\n",
    "    \n",
    "    entry['type'] = 'webpage'\n",
    "    entry['page_links'] = [urljoin(entry['domain_url'], page_url) for page_url in entry['page_links']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_un = [entry['page_links'] for entry in data_un]\n",
    "pdfs_un = list(itertools.chain(*pdfs_un))\n",
    "pdfs_un = [link for link in pdfs_un if link.endswith('pdf')]\n",
    "\n",
    "pdfs_drmkc = [entry['page_links'] for entry in data_drmkc]\n",
    "pdfs_drmkc = list(itertools.chain(*pdfs_drmkc))\n",
    "pdfs_drmkc = [link for link in pdfs_drmkc if link.endswith('pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading pdfs for unddr\n",
      "\n",
      "|==================================================| 100.00 %\n",
      "\n",
      "downloading pdfs for drmkc\n",
      "\n",
      "|==================================================| 100.00 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orgs = ['unddr', 'drmkc']\n",
    "\n",
    "def url_to_filename(url):\n",
    "    url = re.sub(r'(https\\:\\/\\/(www\\.)?)|(http\\:\\/\\/(www\\.)?)', '', url)\n",
    "    url = re.sub(r'\\:\\d{2,4}(?=\\/)', '', url)\n",
    "    urlpart = re.search(r'(\\w+?)\\.\\w{2,11}(\\.\\w{2,5})?(?=\\/)', url).group(1)\n",
    "    namepart = re.search(r'\\.\\w{2,11}(\\/.+\\.pdf)', url).group(1).replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
    "    namepart = namepart.replace(\"?\", \"\")\n",
    "    filename = urlpart + namepart\n",
    "    return(filename)\n",
    "\n",
    "for org in orgs:\n",
    "    \n",
    "    missed_pdfs = []\n",
    "    \n",
    "    save_path = os.path.join(pdf_path, org)\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    \n",
    "    domain_set = [entry for entry in data if entry.get('org') == org]\n",
    "    \n",
    "    pdfs = list(set([url for url in list(itertools.chain(*[list(compress(entry['page_links'], [(\".pdf\" in link) for link in entry['page_links']])) for entry in domain_set]))]))\n",
    "    \n",
    "    print(\"downloading pdfs for {}\\n\".format(org))\n",
    "    for c, pdf_url in enumerate(pdfs, start = 1):\n",
    "    \n",
    "        filename = url_to_filename(pdf_url)\n",
    "        \n",
    "        if os.path.isfile(os.path.join(save_path, filename)):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(pdf_url, stream=True)\n",
    "        except:\n",
    "            missed_pdfs.append(pdf_url)\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            time.sleep(random.uniform(0.5, 1))\n",
    "        else:\n",
    "            missed_pdfs.append(pdf_url)\n",
    "            continue\n",
    "\n",
    "        progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(pdfs) * 50)).ljust(50), c/len(pdfs) * 100)\n",
    "    \n",
    "        print(progress, end = \"\\r\")\n",
    "        \n",
    "        with open(os.path.join(save_path, 'missed_pdf.txt'), 'w', encoding = 'utf-8') as f:\n",
    "            for url in missed_pdfs:\n",
    "                f.write(url + \"\\n\")\n",
    "            f.close()\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|==================================================| 100.00 %\r"
     ]
    }
   ],
   "source": [
    "pdf_data = []\n",
    "problem_files = []\n",
    "\n",
    "for org in orgs:\n",
    "    save_path = os.path.join(pdf_path, org)\n",
    "\n",
    "    filenames = [os.path.join(save_path,f) for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f))]\n",
    "    filenames = [filename for filename in filenames if filename.endswith('.pdf')]\n",
    "\n",
    "    for c, filename in enumerate(filenames, start = 1):\n",
    "\n",
    "        if filename in [entry.get('filename') for entry in pdf_data]:\n",
    "            continue\n",
    "\n",
    "        progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(filenames) * 50)).ljust(50), c/len(filenames) * 100)\n",
    "\n",
    "        entry = {}\n",
    "        entry['filename'] = filename\n",
    "        entry['org'] = org\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(filename) as pdf:\n",
    "                try:\n",
    "                    pdf_text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text() is not None])\n",
    "                    entry['text'] = pdf_text\n",
    "                except Exception as e:\n",
    "                    print(filename)\n",
    "                    raise e\n",
    "        except:\n",
    "            problem_files.append(filename)\n",
    "\n",
    "        pdf_data.append(entry)\n",
    "\n",
    "        print(progress, end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_links = pdfs_un + pdfs_drmkc\n",
    "pdf_link_name = {}\n",
    "\n",
    "for pdf_link in all_pdf_links:\n",
    "    filename = url_to_filename(pdf_link)\n",
    "    pdf_link_name[filename] = pdf_link\n",
    "\n",
    "for entry in pdf_data:\n",
    "    if entry.get('org') == 'unddr':\n",
    "        folder_path = '../data/pdf/unddr/'\n",
    "    elif entry.get('org') == 'drmkc':\n",
    "        folder_path = '../data/pdf/drmkc/'\n",
    "        \n",
    "    filename_lookup = entry.get('filename').replace(folder_path, '')\n",
    "    entry['url'] = pdf_link_name.get(filename_lookup)\n",
    "    entry['type'] = 'pdf'\n",
    "    \n",
    "    if entry.get('org') == 'unddr':\n",
    "        entry['domain_url'] = 'https://www.unddr.org'\n",
    "    elif entry.get('org') == 'drmkc':\n",
    "        entry['domain_url'] = 'https://drmkc.jrc.ec.europa.eu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['domain_url', 'url', 'links', 'date-of-access', 'page_text', 'page_links', 'org', 'type'])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filename', 'org', 'text', 'url', 'type', 'domain_url'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and reorder data\n",
    "\n",
    "data_all_un = [entry for entry in data if entry['org'] == 'unddr'] + [entry for entry in pdf_data if entry['org'] == 'unddr' and 'text' in entry]\n",
    "data_all_drmkc = [entry for entry in data if entry['org'] == 'drmkc'] + [entry for entry in pdf_data if entry['org'] == 'drmkc' and 'text' in entry]\n",
    "\n",
    "data_all = []\n",
    "\n",
    "i = 1\n",
    "for entry in data_all_un:\n",
    "    if ('text' not in entry and 'page_text' in entry):\n",
    "        entry['text'] = entry.pop('page_text')\n",
    "        \n",
    "    if 'page_links' not in entry:\n",
    "        entry['page_links'] = []\n",
    "        \n",
    "    entry['id'] = 'unddr' + str.rjust(str(i), 5, str(0))\n",
    "    i = i +1\n",
    "    \n",
    "    new_entry = {}\n",
    "    new_entry['id'] = entry['id']\n",
    "    new_entry['url'] = entry['url']\n",
    "    new_entry['domain_url'] = entry['domain_url']\n",
    "    new_entry['text'] = entry['text']\n",
    "    new_entry['org'] = entry['org']\n",
    "    new_entry['page_links'] = entry['page_links']\n",
    "    new_entry['type'] = entry['type']\n",
    "    \n",
    "    data_all.append(new_entry)\n",
    "\n",
    "i = 1\n",
    "for entry in data_all_drmkc:\n",
    "    if ('text' not in entry and 'page_text' in entry):\n",
    "        entry['text'] = entry.pop('page_text')\n",
    "    \n",
    "    if 'page_links' not in entry:\n",
    "        entry['page_links'] = []\n",
    "        \n",
    "    entry['id'] = 'drmkc' + str.rjust(str(i), 5, str(0))\n",
    "    i = i +1\n",
    "    \n",
    "    new_entry = {}\n",
    "    new_entry['id'] = entry['id']\n",
    "    new_entry['url'] = entry['url']\n",
    "    new_entry['domain_url'] = entry['domain_url']\n",
    "    try: \n",
    "        new_entry['text'] = entry['text']\n",
    "    except:\n",
    "        print(entry)\n",
    "    new_entry['org'] = entry['org']\n",
    "    new_entry['page_links'] = entry['page_links']\n",
    "    new_entry['type'] = entry['type']\n",
    "    \n",
    "    data_all.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data for ES\n",
    "\n",
    "with open(os.path.join(data_path, datafile_es), 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(data_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\", disable=[\"ner\"])\n",
    "nlp.max_length = 2500000\n",
    "\n",
    "stop_words = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Tokenizer\n",
    "def tokenizer_custom(text, stop_words=stop_words, tags=['NOUN', 'ADJ', 'VERB', 'PROPN']):\n",
    "       \n",
    "    text = text.replace('\\n', ' ')\n",
    "    numbers_re = r\".*\\d.*\"\n",
    "    punct_regex = r\"[^\\w\\s]\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "        \n",
    "    pos_tags = tags # Keeps proper nouns, adjectives and nouns\n",
    "    \n",
    "    tokens = []\n",
    "      \n",
    "    for word in doc:\n",
    "        if (word.pos_ in pos_tags) and (len(word.lemma_) > 4) and (word.lemma_.lower() not in stop_words) and not (re.match(numbers_re, word.lemma_.lower())):\n",
    "            token = word.lemma_.lower() # Returning the word in lower-case.\n",
    "            token = re.sub(punct_regex, \"\", token)\n",
    "            tokens.append(token)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "# Dummy functions for using existing tokens in sklearn vectorizer\n",
    "def return_tokens(tokens):\n",
    "    return tokens\n",
    "\n",
    "# Function for summarizing keywords with tf-idf\n",
    "def tfidf_summarize(token_list, n_words = 50):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=return_tokens,\n",
    "        preprocessor=return_tokens,\n",
    "        token_pattern=None,\n",
    "        norm = False)\n",
    "\n",
    "    # Fitting vectorizer\n",
    "    transformed_documents = vectorizer.fit_transform(token_list)\n",
    "    transformed_documents_as_array = transformed_documents.toarray()\n",
    "    df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "    # Word count\n",
    "    word_tfidfsum = df.sum().sort_values(ascending = False)\n",
    "    word_tfidfsum_select = word_tfidfsum[0:n_words]\n",
    "    \n",
    "    return(word_tfidfsum_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "if not is_tokenized:\n",
    "    for entry in data_all:\n",
    "        entry['tokens'] = tokenizer_custom(entry.get('text'))\n",
    "        \n",
    "    # Save tokenized data\n",
    "    with open(os.path.join(data_path, datafile_tokenized), 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(data_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data consists of 433 texts in total. \n",
      "\n",
      "196 texts are from UN DDR (https://www.unddr.org). 133 from webpages and 63 from pdfs \n",
      "\n",
      "237 texts are from DRMKC EU (https://drmkc.jrc.ec.europa.eu). 151 from webpages and 86 from pdfs\n"
     ]
    }
   ],
   "source": [
    "data_un = [entry for entry in data_all if entry['org'] == 'unddr']\n",
    "data_drmkc = [entry for entry in data_all if entry['org'] == 'drmkc']\n",
    "\n",
    "print(f\"\"\"The data consists of {len(data_all)} texts in total. \\n\n",
    "{len(data_un)} texts are from UN DDR (https://www.unddr.org). {dict(Counter([entry.get('type') for entry in data_un]))['webpage']} from webpages and {dict(Counter([entry.get('type') for entry in data_un]))['pdf']} from pdfs \\n\n",
    "{len(data_drmkc)} texts are from DRMKC EU (https://drmkc.jrc.ec.europa.eu). {dict(Counter([entry.get('type') for entry in data_drmkc]))['webpage']} from webpages and {dict(Counter([entry.get('type') for entry in data_drmkc]))['pdf']} from pdfs\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('should', 6140),\n",
       " ('their', 4689),\n",
       " ('national', 4093),\n",
       " ('support', 3832),\n",
       " ('reintegration', 3675),\n",
       " ('international', 3664),\n",
       " ('management', 3532),\n",
       " ('disaster', 3504),\n",
       " ('security', 3323),\n",
       " ('information', 3290),\n",
       " ('groups', 3288),\n",
       " ('programmes', 3161),\n",
       " ('programme', 3071),\n",
       " ('european', 2836),\n",
       " ('processes', 2479),\n",
       " ('system', 2450),\n",
       " ('these', 2422),\n",
       " ('force', 2364),\n",
       " ('service', 2200),\n",
       " ('including', 2156),\n",
       " ('development', 2063),\n",
       " ('commission', 2016),\n",
       " ('crisis', 2007),\n",
       " ('community', 2002),\n",
       " ('demobilization', 1976),\n",
       " ('planning', 1976),\n",
       " ('activities', 1960),\n",
       " ('peace', 1905),\n",
       " ('measures', 1884),\n",
       " ('people', 1883),\n",
       " ('women', 1882),\n",
       " ('process', 1875),\n",
       " ('weapons', 1825),\n",
       " ('united', 1798),\n",
       " ('between', 1784),\n",
       " ('natural', 1756),\n",
       " ('assessment', 1716),\n",
       " ('excombatants', 1693),\n",
       " ('during', 1692),\n",
       " ('transitional', 1630),\n",
       " ('provide', 1612),\n",
       " ('health', 1594),\n",
       " ('inform', 1592),\n",
       " ('research', 1579),\n",
       " ('communities', 1573),\n",
       " ('different', 1560),\n",
       " ('disarmament', 1522),\n",
       " ('framework', 1514),\n",
       " ('impacts', 1512),\n",
       " ('human', 1511)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keywords based on counts\n",
    "\n",
    "drr_tokens = [entry['tokens'] for entry in data_all]\n",
    "drr_tokens_flat = list(itertools.chain(*drr_tokens))\n",
    "\n",
    "un_tokens = [entry['tokens'] for entry in data_un]\n",
    "un_tokens_flat = list(itertools.chain(*un_tokens))\n",
    "\n",
    "drmkc_tokens = [entry['tokens'] for entry in data_drmkc]\n",
    "drmkc_tokens_flat = list(itertools.chain(*drmkc_tokens))\n",
    "\n",
    "tokens_counted = Counter(drr_tokens_flat)\n",
    "tokens_counted_un = Counter(un_tokens_flat)\n",
    "tokens_counted_drmkc = Counter(drmkc_tokens_flat)\n",
    "tokens_counted.most_common()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('should', 5109),\n",
       " ('reintegration', 3675),\n",
       " ('their', 3373),\n",
       " ('support', 3177),\n",
       " ('programmes', 3039),\n",
       " ('national', 3038),\n",
       " ('programme', 2892),\n",
       " ('security', 2864),\n",
       " ('groups', 2819),\n",
       " ('international', 2321),\n",
       " ('force', 2298),\n",
       " ('processes', 2126),\n",
       " ('demobilization', 1976),\n",
       " ('peace', 1895),\n",
       " ('information', 1872),\n",
       " ('women', 1872),\n",
       " ('weapons', 1822),\n",
       " ('excombatants', 1693),\n",
       " ('including', 1673),\n",
       " ('transitional', 1626),\n",
       " ('process', 1556),\n",
       " ('activities', 1543),\n",
       " ('disarmament', 1522),\n",
       " ('community', 1519),\n",
       " ('these', 1519),\n",
       " ('planning', 1468),\n",
       " ('mission', 1429),\n",
       " ('rights', 1337),\n",
       " ('ensure', 1264),\n",
       " ('ammunition', 1237),\n",
       " ('development', 1234),\n",
       " ('integrated', 1196),\n",
       " ('provide', 1192),\n",
       " ('communities', 1186),\n",
       " ('measures', 1144),\n",
       " ('combatants', 1135),\n",
       " ('nations', 1122),\n",
       " ('include', 1105),\n",
       " ('during', 1080),\n",
       " ('management', 1067),\n",
       " ('where', 1053),\n",
       " ('training', 1048),\n",
       " ('united', 1036),\n",
       " ('political', 1031),\n",
       " ('service', 996),\n",
       " ('between', 939),\n",
       " ('gender', 939),\n",
       " ('violence', 938),\n",
       " ('needs', 938),\n",
       " ('assistance', 933)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_counted_un.most_common()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disaster', 3495),\n",
       " ('european', 2799),\n",
       " ('management', 2465),\n",
       " ('crisis', 1936),\n",
       " ('system', 1769),\n",
       " ('inform', 1495),\n",
       " ('commission', 1473),\n",
       " ('impacts', 1444),\n",
       " ('research', 1438),\n",
       " ('information', 1418),\n",
       " ('international', 1343),\n",
       " ('their', 1316),\n",
       " ('people', 1302),\n",
       " ('service', 1204),\n",
       " ('assessment', 1182),\n",
       " ('disasters', 1181),\n",
       " ('change', 1172),\n",
       " ('global', 1143),\n",
       " ('natural', 1127),\n",
       " ('damage', 1118),\n",
       " ('hazards', 1066),\n",
       " ('national', 1055),\n",
       " ('medium', 1035),\n",
       " ('should', 1031),\n",
       " ('index', 1010),\n",
       " ('different', 1003),\n",
       " ('science', 997),\n",
       " ('knowledge', 962),\n",
       " ('impact', 962),\n",
       " ('communication', 945),\n",
       " ('events', 942),\n",
       " ('centre', 920),\n",
       " ('model', 915),\n",
       " ('these', 903),\n",
       " ('reduction', 881),\n",
       " ('vulnerability', 881),\n",
       " ('complex', 860),\n",
       " ('severity', 853),\n",
       " ('between', 845),\n",
       " ('infrastructure', 844),\n",
       " ('africa', 842),\n",
       " ('development', 829),\n",
       " ('resilience', 826),\n",
       " ('population', 808),\n",
       " ('health', 804),\n",
       " ('report', 795),\n",
       " ('university', 790),\n",
       " ('framework', 781),\n",
       " ('protection', 775),\n",
       " ('europe', 774)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_counted_drmkc.most_common()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "should            14188.745480\n",
       "their              9765.367421\n",
       "reintegration      9666.695295\n",
       "disaster           8864.243130\n",
       "national           8441.441019\n",
       "support            7802.304282\n",
       "groups             7771.235126\n",
       "programmes         7471.068806\n",
       "international      7320.198314\n",
       "programme          7258.352516\n",
       "management         7237.647295\n",
       "security           7154.630370\n",
       "european           6807.011724\n",
       "information        6593.621692\n",
       "force              6274.538082\n",
       "processes          5544.934553\n",
       "these              5436.737990\n",
       "system             5257.187580\n",
       "demobilization     5244.706958\n",
       "women              5136.428009\n",
       "weapons            5103.390608\n",
       "service            4973.892393\n",
       "peace              4944.823206\n",
       "excombatants       4933.675336\n",
       "crisis             4817.232909\n",
       "including          4771.728472\n",
       "natural            4703.613486\n",
       "commission         4641.557578\n",
       "community          4626.362940\n",
       "transitional       4558.096817\n",
       "activities         4546.130512\n",
       "planning           4532.787977\n",
       "measures           4419.217252\n",
       "development        4397.082556\n",
       "process            4348.976893\n",
       "people             4272.571230\n",
       "disarmament        4039.698376\n",
       "between            4018.929136\n",
       "impacts            3977.154636\n",
       "assessment         3965.453948\n",
       "inform             3944.856097\n",
       "united             3911.018112\n",
       "during             3909.993054\n",
       "ammunition         3827.319065\n",
       "mission            3796.495593\n",
       "communities        3790.309357\n",
       "rights             3787.963875\n",
       "ensure             3779.446129\n",
       "research           3717.827602\n",
       "health             3711.011175\n",
       "dtype: float64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keywords based on TF-IDF\n",
    "\n",
    "tfidf_summarize(drr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "should            10395.325658\n",
       "reintegration      6764.030336\n",
       "their              6585.682620\n",
       "support            5923.081635\n",
       "national           5813.990352\n",
       "programmes         5702.636339\n",
       "groups             5617.546147\n",
       "programme          5571.423257\n",
       "security           5480.996830\n",
       "force              4810.975571\n",
       "international      4657.173792\n",
       "processes          4015.492224\n",
       "women              3756.238405\n",
       "weapons            3733.455142\n",
       "demobilization     3683.981527\n",
       "information        3679.819632\n",
       "peace              3602.730593\n",
       "excombatants       3596.474853\n",
       "transitional       3331.832086\n",
       "including          3288.642224\n",
       "process            3188.395281\n",
       "activities         3161.757017\n",
       "these              3090.722191\n",
       "community          3090.722191\n",
       "darfur             3034.839366\n",
       "mission            2887.328584\n",
       "planning           2885.670523\n",
       "ammunition         2884.634533\n",
       "disarmament        2837.560670\n",
       "rights             2819.487605\n",
       "ensure             2665.544004\n",
       "measures           2504.054886\n",
       "communities        2465.114176\n",
       "development        2442.257076\n",
       "provide            2359.133254\n",
       "combatants         2359.110109\n",
       "include            2313.371630\n",
       "integrated         2304.087903\n",
       "service            2196.843620\n",
       "during             2167.060618\n",
       "management         2126.258155\n",
       "training           2117.508997\n",
       "political          2112.619238\n",
       "police             2108.387888\n",
       "nations            2078.378135\n",
       "where              2055.951319\n",
       "human              2018.094444\n",
       "assistance         1996.684379\n",
       "between            1965.842499\n",
       "needs              1935.746870\n",
       "dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_summarize(un_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disaster          7052.642204\n",
       "european          5552.192417\n",
       "management        5152.451556\n",
       "crisis            4171.659036\n",
       "system            3933.858739\n",
       "impacts           3482.679146\n",
       "inform            3390.029191\n",
       "commission        3193.766820\n",
       "research          3060.733032\n",
       "change            3000.610267\n",
       "people            2991.844192\n",
       "their             2889.415594\n",
       "information       2878.000334\n",
       "natural           2819.730063\n",
       "service           2766.651618\n",
       "should            2681.702822\n",
       "international     2664.020870\n",
       "assessment        2662.765829\n",
       "damage            2658.521393\n",
       "global            2644.202016\n",
       "disasters         2643.271884\n",
       "hazards           2552.785623\n",
       "communication     2540.236583\n",
       "science           2532.823706\n",
       "medium            2532.561147\n",
       "vulnerability     2329.037393\n",
       "national          2316.362805\n",
       "journal           2315.031774\n",
       "model             2306.739664\n",
       "complex           2292.423038\n",
       "index             2275.290598\n",
       "different         2274.380788\n",
       "severity          2273.763781\n",
       "events            2271.941659\n",
       "these             2209.567842\n",
       "infrastructure    2195.302795\n",
       "university        2179.371900\n",
       "africa            2172.739216\n",
       "resilience        2148.483541\n",
       "knowledge         2139.271965\n",
       "reduction         2124.820171\n",
       "function          2087.524330\n",
       "centre            2072.541931\n",
       "health            2058.439125\n",
       "between           2023.549579\n",
       "population        2006.495595\n",
       "impact            1987.060472\n",
       "chapter           1975.739564\n",
       "heritage          1960.646223\n",
       "stable            1951.241066\n",
       "dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_summarize(drmkc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "\n",
    "## Dictionary and filter extremes\n",
    "id2token = corpora.Dictionary([entry.get('tokens') for entry in data_all])\n",
    "id2token.filter_extremes(no_below=0.05, no_above=0.95)\n",
    "\n",
    "## Gensim doc2bow corpus\n",
    "for entry in data_all:\n",
    "    entry['doc2bow'] = id2token.doc2bow(entry.get('tokens'))    \n",
    "    \n",
    "tokens_bow = [entry.get('doc2bow') for entry in data_all]\n",
    "\n",
    "## LDA model\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus = tokens_bow, \n",
    "                                       num_topics = 10, \n",
    "                                       id2word = id2token, \n",
    "                                       chunksize = 1000, \n",
    "                                       passes = 20, \n",
    "                                       workers = 4, \n",
    "                                       iterations = 2000, \n",
    "                                       random_state = 1332)\n",
    "\n",
    "## Save model\n",
    "lda_model.save(os.path.join(out_path, 'lda_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -0.7038474234374916\n"
     ]
    }
   ],
   "source": [
    "## Compute Coherence Score - https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, corpus=tokens_bow, coherence='u_mass')\n",
    "\n",
    "coherence_ldamodel = coherence_model_lda.get_coherence() \n",
    "print('\\nCoherence Score: ', coherence_ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('disaster', 0.01122476),\n",
      "   ('management', 0.007977575),\n",
      "   ('european', 0.0079092765),\n",
      "   ('system', 0.005714025),\n",
      "   ('impacts', 0.0048752055),\n",
      "   ('their', 0.004089153),\n",
      "   ('change', 0.003980041),\n",
      "   ('disasters', 0.0038803057),\n",
      "   ('information', 0.003842459),\n",
      "   ('research', 0.0038103017)]),\n",
      " (1,\n",
      "  [('function', 0.018646052),\n",
      "   ('newcategorysectionsettingscategorysection', 0.015501083),\n",
      "   ('european', 0.011650168),\n",
      "   ('return', 0.011440888),\n",
      "   ('commission', 0.010161978),\n",
      "   ('found', 0.009384863),\n",
      "   ('university', 0.009039401),\n",
      "   ('research', 0.008704226),\n",
      "   ('centre', 0.008032969),\n",
      "   ('joint', 0.0071320636)]),\n",
      " (2,\n",
      "  [('inform', 0.028230362),\n",
      "   ('index', 0.017968168),\n",
      "   ('severity', 0.015301224),\n",
      "   ('crisis', 0.011634377),\n",
      "   ('people', 0.008789542),\n",
      "   ('dimension', 0.0065143546),\n",
      "   ('population', 0.0063117063),\n",
      "   ('indicators', 0.006056953),\n",
      "   ('model', 0.0054247826),\n",
      "   ('vulnerability', 0.0050228564)]),\n",
      " (3,\n",
      "  [('reintegration', 0.008804386),\n",
      "   ('natural', 0.008298727),\n",
      "   ('transitional', 0.006805207),\n",
      "   ('excombatants', 0.0064840415),\n",
      "   ('women', 0.0064297267),\n",
      "   ('processes', 0.0062312484),\n",
      "   ('their', 0.0061701154),\n",
      "   ('measures', 0.0052329735),\n",
      "   ('resources', 0.004700761),\n",
      "   ('programmes', 0.004556504)]),\n",
      " (4,\n",
      "  [('political', 0.020809168),\n",
      "   ('peace', 0.015362138),\n",
      "   ('groups', 0.010379431),\n",
      "   ('party', 0.009681571),\n",
      "   ('agreements', 0.008419246),\n",
      "   ('process', 0.007972355),\n",
      "   ('processes', 0.007656745),\n",
      "   ('security', 0.0072692744),\n",
      "   ('support', 0.0061408808),\n",
      "   ('their', 0.0058007687)]),\n",
      " (5,\n",
      "  [('should', 0.012910303),\n",
      "   ('their', 0.009746704),\n",
      "   ('national', 0.009121665),\n",
      "   ('security', 0.008510588),\n",
      "   ('support', 0.007445726),\n",
      "   ('weapons', 0.0074379933),\n",
      "   ('groups', 0.007361141),\n",
      "   ('international', 0.007240305),\n",
      "   ('reintegration', 0.0068457862),\n",
      "   ('force', 0.0062966184)]),\n",
      " (6,\n",
      "  [('darfur', 0.01574348),\n",
      "   ('unamid', 0.011957723),\n",
      "   ('north', 0.004458772),\n",
      "   ('clips', 0.004242103),\n",
      "   ('voices', 0.00423271),\n",
      "   ('projects', 0.004076089),\n",
      "   ('august', 0.0035801758),\n",
      "   ('darfuri', 0.0032943746),\n",
      "   ('tayeb', 0.003184912),\n",
      "   ('adekoya', 0.0029269555)]),\n",
      " (7,\n",
      "  [('should', 0.014581957),\n",
      "   ('programme', 0.012205478),\n",
      "   ('programmes', 0.012100976),\n",
      "   ('reintegration', 0.011162087),\n",
      "   ('support', 0.009632141),\n",
      "   ('demobilization', 0.007360209),\n",
      "   ('national', 0.0064081512),\n",
      "   ('groups', 0.006284259),\n",
      "   ('information', 0.005906183),\n",
      "   ('security', 0.0057948185)]),\n",
      " (8,\n",
      "  [('crisis', 0.035030577),\n",
      "   ('medium', 0.028419739),\n",
      "   ('complex', 0.026642952),\n",
      "   ('africa', 0.02466639),\n",
      "   ('international', 0.022475878),\n",
      "   ('stable', 0.02117655),\n",
      "   ('displacement', 0.015591239),\n",
      "   ('increasing', 0.012872733),\n",
      "   ('conflict', 0.011646599),\n",
      "   ('multiple', 0.008308646)]),\n",
      " (9,\n",
      "  [('reintegration', 0.010788379),\n",
      "   ('women', 0.007282469),\n",
      "   ('transitional', 0.0072170687),\n",
      "   ('excombatants', 0.0071795327),\n",
      "   ('processes', 0.0070756725),\n",
      "   ('their', 0.0065204673),\n",
      "   ('programmes', 0.005586215),\n",
      "   ('measures', 0.005466786),\n",
      "   ('should', 0.0054646167),\n",
      "   ('locally', 0.004855146)])]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "# Show Topics\n",
    "pprint(lda_model.show_topics(formatted=False, num_topics=15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8, tm",
   "language": "python",
   "name": "tmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
