{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8910c6e66f64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "import re\n",
    "import itertools\n",
    "from itertools import compress\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(20,12)})\n",
    "\n",
    "data_path = os.path.join('')\n",
    "out_path = os.path.join('output')\n",
    "datafile = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4d5ce8f6ebbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "path = os.path.join(data_path, datafile)\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "print(f\"The data consists of {len(data)} texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "stop_words = list(nlp.Defaults.stop_words)\n",
    "                                            \n",
    "def tokenizer_custom(text, stop_words=stop_words, tags=['NOUN', 'ADJ', 'VERB', 'PROPN']):\n",
    "       \n",
    "    text = text.replace('\\n', ' ')\n",
    "    numbers_re = r\".*\\d.*\"\n",
    "    punct_regex = r\"[^\\w\\s]\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "        \n",
    "    pos_tags = tags # Keeps proper nouns, adjectives and nouns\n",
    "    \n",
    "    tokens = []\n",
    "      \n",
    "    for word in doc:\n",
    "        if ((word.pos_ in pos_tags) or (any([exception in word.text for exception in exceptions]))) and (len(word.lemma_) > 4) and (word.lemma_.lower() not in stop_words) and not (re.match(numbers_re, word.lemma_.lower())):\n",
    "            token = word.lemma_.lower() # Returning the word in lower-case.\n",
    "            token = re.sub(punct_regex, \"\", token)\n",
    "            tokens.append(token)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def return_tokens(tokens):\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "\n",
    "for entry in data:\n",
    "    entry['tokens'] = tokenizer_custom(entry.get('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords based on counts\n",
    "\n",
    "drr_tokens = [entry['tokens'] for entry in data]\n",
    "drr_tokens = list(itertools.chain(*drr_tokens))\n",
    "\n",
    "print(Counter(drr_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords based on TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=return_tokens,\n",
    "    preprocessor=return_tokens,\n",
    "    token_pattern=None,\n",
    "    norm = False)\n",
    "\n",
    "# Fitting vectorizer\n",
    "transformed_documents = vectorizer.fit_transform(drr_tokens)\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Word count\n",
    "word_tfidfsum = df.sum().sort_values(ascending = False)\n",
    "word_tfidfsum[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "\n",
    "## Dictionary and filter extremes\n",
    "id2token = corpora.Dictionary([entry.get('tokens') for entry in data])\n",
    "\n",
    "## Gensim doc2bow corpus\n",
    "for entry in data:\n",
    "    entry['doc2bow'] = id2token.doc2bow(entry.get('tokens'))    \n",
    "    \n",
    "tokens_bow = [entry.get('doc2bow') for entry in data]\n",
    "\n",
    "## LDA model\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus = tokens_bow, \n",
    "                                       num_topics = 5, \n",
    "                                       id2word = id2token, \n",
    "                                       chunksize = 1000, \n",
    "                                       passes = 20, \n",
    "                                       workers = 4, \n",
    "                                       iterations = 2000, \n",
    "                                       random_state = 1332)\n",
    "\n",
    "\n",
    "## Compute Coherence Score - https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, corpus=tokens_tfidf, coherence='u_mass')\n",
    "\n",
    "coherence_ldamodel = coherence_model_lda.get_coherence() \n",
    "print('\\nCoherence Score: ', coherence_ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "# Show Topics\n",
    "pprint(lda_model.show_topics(formatted=False, num_topics=15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
